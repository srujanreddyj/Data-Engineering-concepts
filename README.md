# Data-Engineering-Concepts

### Data Engineering skills and tools
* Languages: Python, SQL
*  Databases: Postgres, Mysql, AWS RDS, DynamoDB, RedShift, Apache Cassandra
* Modeling: Dimensional data modeling
* Batch ETL: Python, SparkSQL
* Workflow Management: Airflow

### Data Engineering projects/Concepts Learnt
1. Data Modeling
  * Created a relational and NoSQL data models to fit the diverse needs of data consumers. 
  * Used ETL to build databases in PostgreSQL and Apache Cassandra.
2. Data Warehouses
  * Applied Data Warehouse architectures learnt and built a data warehouse on AWS cloud. 
  * ETL pipeline that extracts data from S3, stages them in Redshift, and transforms data into a set of dimensional tables for an analytics team.
3. Data Lakes
  * Built a data lake on AWS Cloud using Spark and AWS EMR CLuster. The Data lake acts as a single source analytics palform and an ETL jobs are written in Spark that loads data from S3, processes the data into analytics tables, and loads them back into S3.
4. Data Pipelines with Airflow
  * Created and automated a set of data pipelines with Airflow, monitoring and debugging production pipelines. I scheduled ETL jobs in Airflow, create project related  plugins, operators and automate the pipeline execution.

