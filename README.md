# Data-Engineering-Concepts

### Data Engineering skills and tools
* Languages: Python, SQL
*  Databases: Postgres, Mysql, AWS RDS, DynamoDB, RedShift, Apache Cassandra
* Modeling: Dimensional data modeling
* Batch ETL: Python, SparkSQL
* Workflow Management: Airflow

### Data Engineering projects/Concepts Learnt
1. [Data Modeling](https://github.com/srujanreddyj/data-engineering-concepts/tree/master/Postgres-cassandra)
  * Created a relational and NoSQL data models to fit the diverse needs of data consumers. 
  * Used ETL to build databases in PostgreSQL and Apache Cassandra.
2. [Data Warehouses](https://github.com/srujanreddyj/data-engineering-concepts/tree/master/datawarehouse)
  * Applied Data Warehouse architectures learnt and built a data warehouse on AWS cloud. 
  * ETL pipeline that extracts data from S3, stages them in Redshift, and transforms data into a set of dimensional tables for an analytics team.
3. [Data Lakes](https://github.com/srujanreddyj/data-engineering-concepts/tree/master/Datalake)
  * Built a data lake on AWS Cloud using Spark and AWS EMR CLuster. The Data lake acts as a single source analytics palform and an ETL jobs are written in Spark that loads data from S3, processes the data into analytics tables, and loads them back into S3.
4. [Data Pipelines with Airflow](https://github.com/srujanreddyj/data-engineering-concepts/tree/master/airflow)
  * Created and automated a set of data pipelines with Airflow, monitoring and debugging production pipelines. I scheduled ETL jobs in Airflow, create project related  plugins, operators and automate the pipeline execution.

